{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "[View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/01_pytorch_workflow.pdf) | [Watch Video Walkthrough](https://youtu.be/Z_ikDlimN6A?t=15419) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgYkrRCRec0r"
   },
   "source": [
    "# 01. PyTorch Workflow Fundamentals\n",
    "\n",
    "The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discovered patterns to predict the future.\n",
    "\n",
    "There are many ways to do this and many new ways are being discovered all the time.\n",
    "\n",
    "we see if we can build a PyTorch model that learns the pattern of the straight line and matches it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51Ug7Ug123Ip"
   },
   "source": [
    "## What we're going to cover\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png\" width=900 alt=\"a pytorch workflow flowchat\"/>\n",
    "\n",
    "For now, we'll use this workflow to predict a simple straight line but the workflow steps can be repeated and changed depending on the problem you're working on.\n",
    "\n",
    "Specifically, we're going to cover:\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **1. Getting data ready** | Data can be almost anything but to get started we're going to create a simple straight line |\n",
    "| **2. Building a model** | Here we'll create a model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop**. | \n",
    "| **3. Fitting the model to data (training)** | We've got data and a model, now let's let the model (try to) find patterns in the (**training**) data. |\n",
    "| **4. Making predictions and evaluating a model (inference)** | Our model's found patterns in the data, let's compare its findings to the actual (**testing**) data. |\n",
    "| **5. Saving and loading a model** | You may want to use your model elsewhere, or come back to it later, here we'll cover that. |\n",
    "| **6. Putting it all together** | Let's take all of the above and combine it. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGM1dEsYec0u"
   },
   "outputs": [],
   "source": [
    "what_were_covering = {1: \"data (prepare and load)\",\n",
    "    2: \"build model\",\n",
    "    3: \"fitting the model to data (training)\",\n",
    "    4: \"making predictions and evaluating a model (inference)\",\n",
    "    5: \"saving and loading a model\",\n",
    "    6: \"putting it all together\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZT_ikDC-ec0w",
    "outputId": "1f0b19d0-6e96-4cc9-b8e6-7adcb3f1da27"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci_-geIdec0w"
   },
   "source": [
    "## 1. Data (preparing and loading)\n",
    "\n",
    "I want to stress that \"data\" in machine learning can be almost anything you can imagine. A table of numbers (like a big Excel spreadsheet), images of any kind, videos (YouTube has lots of data!), audio files like songs or podcasts, protein structures, text and more.\n",
    "\n",
    "![machine learning is a game of two parts: 1. turn your data into a representative set of numbers and 2. build or pick a model to learn the representation as best as possible](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-machine-learning-a-game-of-two-parts.png)\n",
    "\n",
    "Machine learning is a game of two parts: \n",
    "1. Turn your data, whatever it is, into numbers (a representation).\n",
    "2. Pick or build a model to learn the representation as best as possible.\n",
    "\n",
    "Sometimes one and two can be done at the same time.\n",
    "\n",
    "Let's create our data as a straight line.\n",
    "\n",
    "We'll use [linear regression](https://en.wikipedia.org/wiki/Linear_regression) to create the data with known **parameters** (things that can be learned by a model) and then we'll use PyTorch to see if we can build model to estimate these parameters using [**gradient descent**](https://en.wikipedia.org/wiki/Gradient_descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmZWVNjGec0x",
    "outputId": "ef7c9d50-31d6-47b6-add9-2cd51694298f"
   },
   "outputs": [],
   "source": [
    "# Create *known* parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "\n",
    "X[:10], y[:10], X.shape, len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzNigr8dtW2Y"
   },
   "source": [
    "Beautiful! Now we're going to move towards building a model that can learn the relationship between `X` (**features**) and `y` (**labels**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YApM7diprjP0"
   },
   "source": [
    "### Split data into training and test sets \n",
    "\n",
    "Each split of the dataset serves a specific purpose:\n",
    "\n",
    "| Split | Purpose | Amount of total data | How often is it used? |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Training set** | The model learns from this data (like the course materials you study during the semester). | ~60-80% | Always |\n",
    "| **Validation set** | The model gets tuned on this data (like the practice exam you take before the final exam). | ~10-20% | Often but not always |\n",
    "| **Testing set** | The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). | ~10-20% | Always |\n",
    "\n",
    "For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.\n",
    "\n",
    "We can create them by splitting our `X` and `y` tensors.\n",
    "\n",
    "> **Note:** When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn from training data and then evaluate it on test data to get an indication of how well it **generalizes** to unseen examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpyB7JgHec0y",
    "outputId": "a859f5c1-37ed-4a9a-b139-20a1107077ed"
   },
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:] \n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9Ep0T-Dec0y"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train, \n",
    "                     test_data=X_test, \n",
    "                     test_labels=y_test, \n",
    "                     predictions=None):\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "  \n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "  if predictions is not None:\n",
    "    # Plot the predictions in red (predictions were made on the test data)\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "  # Show the legend\n",
    "  plt.legend(prop={\"size\": 14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "xTaIwydGec0z",
    "outputId": "0d02d134-f6de-4e6f-c904-b081c7d6b8b1"
   },
   "outputs": [],
   "source": [
    "plot_predictions();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eFsorRHec00"
   },
   "source": [
    "## 2. Build model\n",
    "\n",
    "Now we've got some data, let's build a model to use the blue dots to predict the green dots.\n",
    "\n",
    "Let's replicate a standard linear regression model using pure PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhcUJBFuec00"
   },
   "outputs": [],
   "source": [
    "# Create a Linear Regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhu5wxVO7s_q"
   },
   "source": [
    "Alright there's a fair bit going on above but let's break it down bit by bit.\n",
    "\n",
    "> **Resource:** We'll be using Python classes to create bits and pieces for building neural networks. If you're unfamiliar with Python class notation, I'd recommend reading [Real Python's Object Orientating programming in Python 3 guide](https://realpython.com/python3-object-oriented-programming/) a few times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRRq3a0Gvvnl"
   },
   "source": [
    "### PyTorch model building essentials\n",
    "\n",
    "PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.\n",
    "\n",
    "They are [`torch.nn`](https://pytorch.org/docs/stable/nn.html), [`torch.optim`](https://pytorch.org/docs/stable/optim.html), [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html). For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).\n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torch.nn`](https://pytorch.org/docs/stable/nn.html) | Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). |\n",
    "| [`torch.nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter) | Stores tensors that can be used with `nn.Module`. If `requires_grad=True` gradients (used for updating model parameters via [**gradient descent**](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html))  are calculated automatically, this is often referred to as \"autograd\".  | \n",
    "| [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) | The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass `nn.Module`. Requires a `forward()` method be implemented. | \n",
    "| [`torch.optim`](https://pytorch.org/docs/stable/optim.html) | Contains various optimization algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss). | \n",
    "| `def forward()` | All `nn.Module` subclasses require a `forward()` method, this defines the computation that will take place on the data passed to the particular `nn.Module` (e.g. the linear regression formula above). |\n",
    "\n",
    "If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from `torch.nn`,\n",
    "* `nn.Module` contains the larger building blocks (layers)\n",
    "* `nn.Parameter` contains the smaller parameters like weights and biases (put these together to make `nn.Module`(s))\n",
    "* `forward()` tells the larger blocks how to make calculations on inputs (tensors full of data) within  `nn.Module`(s)\n",
    "* `torch.optim` contains optimization methods on how to improve the parameters within `nn.Parameter` to better represent input data \n",
    "\n",
    "![a pytorch linear model with annotations](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-model-annotated.png)\n",
    "*Basic building blocks of creating a PyTorch model by subclassing `nn.Module`. For objects that subclass `nn.Module`, the `forward()` method must be defined.*\n",
    "\n",
    "> **Resource:** See more of these essential modules and their use cases in the [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYt5sKsgufG7"
   },
   "source": [
    "\n",
    "### Checking the contents of a PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsEKA3A_ec01",
    "outputId": "cd999f12-2efd-4fe7-e449-d51ff98e5242"
   },
   "outputs": [],
   "source": [
    "# Set manual seed since nn.Parameter are randomly initialized\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check the nn.Parameter(s) within the nn.Module subclass we created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNOmcQdSq34e"
   },
   "source": [
    "We can also get the state (what the model contains) of the model using [`.state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XC1N_1Qrec01",
    "outputId": "7e35b61c-371e-4d28-ae02-c1981afc1bbb"
   },
   "outputs": [],
   "source": [
    "# List named parameters \n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDKdLN7nuheb"
   },
   "source": [
    "### Making predictions using `torch.inference_mode()` \n",
    "To check this we can pass it the test data `X_test` to see how closely it predicts `y_test`.\n",
    "\n",
    "When we pass data to our model, it'll go through the model's `forward()` method and produce a result using the computation we've defined. \n",
    "\n",
    "Let's make some predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ITlZgU5ec02"
   },
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "# Note: in older PyTorch code you might also see torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#   y_preds = model_0(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_Bx5I1FsIS0"
   },
   "source": [
    "Hmm?\n",
    "\n",
    "You probably noticed we used [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html) as a [context manager](https://realpython.com/python-with-statement/) (that's what the `with torch.inference_mode():` is) to make the predictions.\n",
    "\n",
    "As the name suggests, `torch.inference_mode()` is used when using a model for inference (making predictions).\n",
    "\n",
    "`torch.inference_mode()` turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make **forward-passes** (data going through the `forward()` method) faster.\n",
    "\n",
    "> **Note:** In older PyTorch code, you may also see `torch.no_grad()` being used for inference. While `torch.inference_mode()` and `torch.no_grad()` do similar things,\n",
    "`torch.inference_mode()` is newer, potentially faster and preferred. See this [Tweet from PyTorch](https://twitter.com/PyTorch/status/1437838231505096708?s=20) for more.\n",
    "\n",
    "We've made some predictions, let's see what they look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4xCScCvec02",
    "outputId": "2ce37ea3-6bc4-4e50-91ef-dcf53277dde7"
   },
   "outputs": [],
   "source": [
    "# Check the predictions\n",
    "print(f\"Number of testing samples: {len(X_test)}\") \n",
    "print(f\"Number of predictions made: {len(y_preds)}\")\n",
    "print(f\"Predicted values:\\n{y_preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnSwGbQEupZs"
   },
   "source": [
    "Notice how there's one prediction value per testing sample.\n",
    "\n",
    "This is because of the kind of data we're using. For our straight line, one `X` value maps to one `y` value. \n",
    "\n",
    "However, machine learning models are very flexible. You could have 100 `X` values mapping to one, two, three or 10 `y` values. It all depends on what you're working on.\n",
    "\n",
    "Our predictions are still numbers on a page, let's visualize them with our `plot_predictions()` function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "pwjxLWZTec02",
    "outputId": "56bf8a4d-2365-4539-a8b7-9bfe606f5b93"
   },
   "outputs": [],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLJWVANkhY3-",
    "outputId": "ed29f680-d66f-4bbd-b1b3-b35655ca4fec"
   },
   "outputs": [],
   "source": [
    "y_test - y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZpa-fXLec03"
   },
   "source": [
    "## 3. Train model\n",
    "\n",
    "Right now the model makes predictions using randomly initialized parameters, so the outputs are effectively guesses.\n",
    "\n",
    "To improve this, we update the model’s internal parameters — **weights** and **bias** — which were initialized using `nn.Parameter()` and `torch.randn()`, so that they better represent the data.\n",
    "\n",
    "In most cases, the ideal parameter values are unknown and cannot be hard-coded.\n",
    "\n",
    "Instead, the model is trained to learn appropriate parameter values automatically from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD8pnhJUyZUT"
   },
   "source": [
    "### Creating a loss function and optimizer in PyTorch\n",
    "\n",
    "For the model to update its parameters during training, two components are required:\n",
    "\n",
    "* a **loss function**\n",
    "* an **optimizer**\n",
    "\n",
    "Their roles are:\n",
    "\n",
    "| Function          | What does it do?                                                                                                                  | Where does it live in PyTorch?           | Common values                                                                                                                         |\n",
    "| ----------------- | --------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Loss function** | Measures how incorrect the model’s predictions (e.g. `y_preds`) are compared to the true labels (e.g. `y_test`). Lower is better. | Built-in loss functions in `torch.nn`    | Mean absolute error (MAE) for regression (`torch.nn.L1Loss()`), Binary cross entropy for binary classification (`torch.nn.BCELoss()`) |\n",
    "| **Optimizer**     | Updates the model’s parameters to minimize the loss.                                                                              | Optimization algorithms in `torch.optim` | Stochastic gradient descent (`torch.optim.SGD()`), Adam optimizer (`torch.optim.Adam()`)                                              |\n",
    "\n",
    "The choice of loss function and optimizer depends on the type of problem.\n",
    "\n",
    "For **regression problems** (predicting a numerical value), MAE (`torch.nn.L1Loss()`) is commonly used.\n",
    "\n",
    "For **classification problems**, binary cross entropy is commonly used.\n",
    "\n",
    "For this problem, since a numerical value is being predicted, MAE is used as the loss function.\n",
    "\n",
    "![what MAE loss looks like for our plot data](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-mae-loss-annotated.png)\n",
    "*Mean absolute error (MAE, in PyTorch: `torch.nn.L1Loss`) measures the absolute difference between predictions and labels, then averages this difference across all examples.*\n",
    "\n",
    "The SGD optimizer is used as `torch.optim.SGD(params, lr)` where:\n",
    "\n",
    "* `params` are the model parameters to optimize (e.g. weights and bias).\n",
    "* `lr` (**learning rate**) controls how large each parameter update is.\n",
    "\n",
    "  * Higher values result in larger updates and faster learning but can overshoot optimal values.\n",
    "  * Lower values result in smaller updates and slower but more stable learning.\n",
    "  * Common starting values are `0.01`, `0.001`, and `0.0001`.\n",
    "  * The learning rate is a **hyperparameter** and can be adjusted during training using learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3T7hpNPec03"
   },
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n",
    "                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFcKCsPcRfnA"
   },
   "source": [
    "### Creating an optimization loop in PyTorch\n",
    "\n",
    "Woohoo! Now we've got a loss function and an optimizer, it's now time to create a **training loop** (and **testing loop**).\n",
    "\n",
    "The training loop involves the model going through the training data and learning the relationships between the `features` and `labels`.\n",
    "\n",
    "The testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never sees the testing data during training).\n",
    "\n",
    "Each of these is called a \"loop\" because we want our model to look (loop through) at each sample in each dataset.\n",
    "\n",
    "To create these we're going to write a Python `for` loop in the theme of the [unofficial PyTorch optimization loop song](https://twitter.com/mrdbourke/status/1450977868406673410?s=20) (there's a [video version too](https://youtu.be/Nutpusq_AFw)).\n",
    "\n",
    "![the unofficial pytorch optimization loop song](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-optimization-loop-song.png)\n",
    "*The unofficial PyTorch optimization loops song, a fun way to remember the steps in a PyTorch training (and testing) loop.*\n",
    "\n",
    "There will be a fair bit of code but nothing we can't handle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agXn72H-sgyd"
   },
   "source": [
    "\n",
    "\n",
    "### PyTorch training loop\n",
    "For the training loop, we'll build the following steps:\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the training data once, performing its `forward()` function calculations. | `model(x_train)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_train)` | \n",
    "| 3 | Zero gradients | The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. | `optimizer.zero_grad()` |\n",
    "| 4 | Perform backpropagation on the loss | Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with `requires_grad=True`). This is known as **backpropagation**, hence \"backwards\".  | `loss.backward()` |\n",
    "| 5 | Update the optimizer (**gradient descent**) | Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them. | `optimizer.step()` |\n",
    "\n",
    "![pytorch training loop annotated](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-training-loop-annotated.png)\n",
    "\n",
    "> **Note:** The above is just one example of how the steps could be ordered or described. With experience you'll find making PyTorch training loops can be quite flexible.\n",
    ">\n",
    "> And on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb: \n",
    "> * Calculate the loss (`loss = ...`) *before* performing backpropagation on it (`loss.backward()`).\n",
    "> * Zero gradients (`optimizer.zero_grad()`) *before* computing the gradients of the loss with respect to every model parameter (`loss.backward()`).\n",
    "> * Step the optimizer (`optimizer.step()`) *after* performing backpropagation on the loss (`loss.backward()`).\n",
    "\n",
    "For resources to help understand what's happening behind the scenes with backpropagation and gradient descent, see the extra-curriculum section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXHDdlfjssDc"
   },
   "source": [
    "\n",
    "### PyTorch testing loop\n",
    "\n",
    "As for the testing loop (evaluating our model), the typical steps include:\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the testing data once, performing its `forward()` function calculations. | `model(x_test)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_test)` | \n",
    "| 3 | Calulate evaluation metrics (optional) | Alongside the loss value you may want to calculate other evaluation metrics such as accuracy on the test set. | Custom functions |\n",
    "\n",
    "Notice the testing loop doesn't contain performing backpropagation (`loss.backward()`) or stepping the optimizer (`optimizer.step()`), this is because no parameters in the model are being changed during testing, they've already been calculated. For testing, we're only interested in the output of the forward pass through the model.\n",
    "\n",
    "![pytorch annotated testing loop](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-testing-loop-annotated.png)\n",
    "\n",
    "Let's put all of the above together and train our model for 100 **epochs** (forward passes through the data) and we'll evaluate it every 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1DfhyJ7ec03",
    "outputId": "333f9780-c103-4e81-95da-9f721c80b617"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs (how many times the model will pass over the training data)\n",
    "epochs = 100\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "\n",
    "    # Put model in training mode (this is the default state of a model)\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass on train data using the forward() method inside \n",
    "    y_pred = model_0(X_train)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Progress the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_0(X_test)\n",
    "\n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1krgBqXBdYHc"
   },
   "source": [
    "Oh would you look at that! Looks like our loss is going down with every epoch, let's plot it to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "FPXfvPLkau72",
    "outputId": "2f6b88b4-4c8e-48ad-eb99-27abd941993d"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmqQE8Kpec04"
   },
   "source": [
    "Nice! The **loss curves** show the loss going down over time. Remember, loss is the measure of how *wrong* your model is, so the lower the better.\n",
    "\n",
    "But why did the loss go down?\n",
    "\n",
    "Well, thanks to our loss function and optimizer, the model's internal parameters (`weights` and `bias`) were updated to better reflect the underlying patterns in the data.\n",
    "\n",
    "Let's inspect our model's [`.state_dict()`](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) to see how close our model gets to the original values we set for weights and bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ci0W7kn5ec04",
    "outputId": "2c27ba8b-e388-484e-c59e-464fdb53d73e"
   },
   "outputs": [],
   "source": [
    "# Find our model's learned parameters\n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "print(model_0.state_dict())\n",
    "print(\"\\nAnd the original values for weights and bias are:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZyBa9rMelBv"
   },
   "source": [
    "Wow! How cool is that?\n",
    "\n",
    "Our model got very close to calculating the exact original values for `weight` and `bias` (and it would probably get even closer if we trained it for longer).\n",
    "\n",
    "> **Exercise:** Try changing the `epochs` value above to 200, what happens to the loss curves and the weights and bias parameter values of the model?\n",
    "\n",
    "It'd likely never guess them *perfectly* (especially when using more complicated datasets) but that's okay, often you can do very cool things with a close approximation.\n",
    "\n",
    "This is the whole idea of machine learning and deep learning, **there are some ideal values that describe our data** and rather than figuring them out by hand, **we can train a model to figure them out programmatically**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-VBDFd2ec05"
   },
   "source": [
    "## 4. Making predictions with a trained PyTorch model (inference)\n",
    "\n",
    "Once you've trained a model, you'll likely want to make predictions with it.\n",
    "\n",
    "We've already seen a glimpse of this in the training and testing code above, the steps to do it outside of the training/testing loop are similar.\n",
    "\n",
    "There are three things to remember when making predictions (also called performing inference) with a PyTorch model:\n",
    "\n",
    "1. Set the model in evaluation mode (`model.eval()`).\n",
    "2. Make the predictions using the inference mode context manager (`with torch.inference_mode(): ...`).\n",
    "3. All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).\n",
    "\n",
    "The first two items make sure all helpful calculations and settings PyTorch uses behind the scenes during training but aren't necessary for inference are turned off (this results in faster computation). And the third ensures that you won't run into cross-device errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKKxSBVuec05",
    "outputId": "7a637fab-186e-4269-85a7-6dc28ee690e0"
   },
   "outputs": [],
   "source": [
    "# 1. Set the model in evaluation mode\n",
    "model_0.eval()\n",
    "\n",
    "# 2. Setup the inference mode context manager\n",
    "with torch.inference_mode():\n",
    "  # 3. Make sure the calculations are done with the model and data on the same device\n",
    "  # in our case, we haven't setup device-agnostic code yet so our data and model are\n",
    "  # on the CPU by default.\n",
    "  # model_0.to(device)\n",
    "  # X_test = X_test.to(device)\n",
    "  y_preds = model_0(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn21JvzmjbBO"
   },
   "source": [
    "Nice! We've made some predictions with our trained model, now how do they look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "b_kBqpCfec05",
    "outputId": "b2e3870b-dfdf-4dbc-877c-a940cb732859"
   },
   "outputs": [],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEHGrjLgji6E"
   },
   "source": [
    "Woohoo! Those red dots are looking far closer than they were before!\n",
    "\n",
    "Let's get onto saving and reloading a model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NRng9aEec05"
   },
   "source": [
    "## 5. Saving and loading a PyTorch model\n",
    "\n",
    "If you've trained a PyTorch model, chances are you'll want to save it and export it somewhere.\n",
    "\n",
    "As in, you might train it on Google Colab or your local machine with a GPU but you'd like to now export it to some sort of application where others can use it. \n",
    "\n",
    "Or maybe you'd like to save your progress on a model and come back and load it back later.\n",
    "\n",
    "For saving and loading models in PyTorch, there are three main methods you should be aware of (all of below have been taken from the [PyTorch saving and loading models guide](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)):\n",
    "\n",
    "| PyTorch method | What does it do? | \n",
    "| ----- | ----- |\n",
    "| [`torch.save`](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) | Saves a serialized object to disk using Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. Models, tensors and various other Python objects like dictionaries can be saved using `torch.save`.  | \n",
    "| [`torch.load`](https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load) | Uses `pickle`'s unpickling features to deserialize and load pickled Python object files (like models, tensors or dictionaries) into memory. You can also set which device to load the object to (CPU, GPU etc). |\n",
    "| [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)| Loads a model's parameter dictionary (`model.state_dict()`) using a saved `state_dict()` object. | \n",
    "\n",
    "> **Note:** As stated in [Python's `pickle` documentation](https://docs.python.org/3/library/pickle.html), the `pickle` module **is not secure**. That means you should only ever unpickle (load) data you trust. That goes for loading PyTorch models as well. Only ever use saved PyTorch models from sources you trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdAGcH2aec05"
   },
   "source": [
    "### Saving a PyTorch model's `state_dict()`\n",
    "\n",
    "The [recommended way](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference) for saving and loading a model for inference (making predictions) is by saving and loading a model's `state_dict()`.\n",
    "\n",
    "Let's see how we can do that in a few steps:\n",
    "\n",
    "1. We'll create a directory for saving models to called `models` using Python's `pathlib` module.\n",
    "2. We'll create a file path to save the model to.\n",
    "3. We'll call `torch.save(obj, f)` where `obj` is the target model's `state_dict()` and `f` is the filename of where to save the model.\n",
    "\n",
    "> **Note:** It's common convention for PyTorch saved models or objects to end with `.pt` or `.pth`, like `saved_model_01.pth`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsQhY2S2jv90",
    "outputId": "a897070c-a843-4a7c-a06e-e6406206412c"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpQc45zwec06",
    "outputId": "50e1b51b-1b98-41f1-ca36-ce9cb5682064"
   },
   "outputs": [],
   "source": [
    "# Check the saved file path\n",
    "!ls -l models/01_pytorch_workflow_model_0.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFQpRoH5ec06"
   },
   "source": [
    "### Loading a saved PyTorch model's `state_dict()`\n",
    "\n",
    "Since we've now got a saved model `state_dict()` at `models/01_pytorch_workflow_model_0.pth` we can now load it in using `torch.nn.Module.load_state_dict(torch.load(f))` where `f` is the filepath of our saved model `state_dict()`.\n",
    "\n",
    "Why call `torch.load()` inside `torch.nn.Module.load_state_dict()`? \n",
    "\n",
    "Because we only saved the model's `state_dict()` which is a dictionary of learned parameters and not the *entire* model, we first have to load the `state_dict()` with `torch.load()` and then pass that `state_dict()` to a new instance of our model (which is a subclass of `nn.Module`).\n",
    "\n",
    "Why not save the entire model?\n",
    "\n",
    "[Saving the entire model](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model) rather than just the `state_dict()` is more intuitive, however, to quote the PyTorch documentation (italics mine):\n",
    "\n",
    "> The disadvantage of this approach *(saving the whole model)* is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved...\n",
    ">\n",
    "> Because of this, your code can break in various ways when used in other projects or after refactors.\n",
    "\n",
    "So instead, we're using the flexible method of saving and loading just the `state_dict()`, which again is basically a dictionary of model parameters.\n",
    "\n",
    "Let's test it out by creating another instance of `LinearRegressionModel()`, which is a subclass of `torch.nn.Module` and will hence have the in-built method `load_state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xnh3cFDec06",
    "outputId": "7ef66bf8-122e-476a-ee86-b1c388d6167c"
   },
   "outputs": [],
   "source": [
    "# Instantiate a new instance of our model (this will be instantiated with random weights)\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK8PRtY7Qgpz"
   },
   "source": [
    "Excellent! It looks like things matched up.\n",
    "\n",
    "Now to test our loaded model, let's perform inference with it (make predictions) on the test data.\n",
    "\n",
    "Remember the rules for performing inference with PyTorch models?\n",
    "\n",
    "If not, here's a refresher:\n",
    "\n",
    "<details>\n",
    "    <summary>PyTorch inference rules</summary>\n",
    "    <ol>\n",
    "      <li> Set the model in evaluation mode (<code>model.eval()</code>). </li>\n",
    "      <li> Make the predictions using the inference mode context manager (<code>with torch.inference_mode(): ...</code>). </li>\n",
    "      <li> All predictions should be made with objects on the same device (e.g. data and model on GPU only or data and model on CPU only).</li>\n",
    "    </ol> \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps-AuJqkec06"
   },
   "outputs": [],
   "source": [
    "# 1. Put the loaded model into evaluation mode\n",
    "loaded_model_0.eval()\n",
    "\n",
    "# 2. Use the inference mode context manager to make predictions\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e81XpN8WSSqn"
   },
   "source": [
    "Now we've made some predictions with the loaded model, let's see if they're the same as the previous predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "il9gqj6Nec06",
    "outputId": "56210de9-9888-4e90-d2e7-6cd0de47f823"
   },
   "outputs": [],
   "source": [
    "# Compare previous model predictions with loaded model predictions (these should be the same)\n",
    "y_preds == loaded_model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y4ZcxxfNcVu"
   },
   "source": [
    "Nice! \n",
    "\n",
    "It looks like the loaded model predictions are the same as the previous model predictions (predictions made prior to saving). This indicates our model is saving and loading as expected.\n",
    "\n",
    "> **Note:** There are more methods to save and load PyTorch models but I'll leave these for extra-curriculum and further reading. See the [PyTorch guide for saving and loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models) for more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeAITvLXec06"
   },
   "source": [
    "## 6. Putting it all together \n",
    "\n",
    "We've covered a fair bit of ground so far. \n",
    "\n",
    "But once you've had some practice, you'll be performing the above steps like dancing down the street.\n",
    "\n",
    "Speaking of practice, let's put everything we've done so far together. \n",
    "\n",
    "Except this time we'll make our code device agnostic (so if there's a GPU available, it'll use it and if not, it will default to the CPU). \n",
    "\n",
    "There'll be far less commentary in this section than above since what we're going to go through has already been covered.\n",
    "\n",
    "We'll start by importing the standard libraries we need.\n",
    "\n",
    "> **Note:** If you're using Google Colab, to setup a GPU, go to Runtime -> Change runtime type -> Hardware acceleration -> GPU. If you do this, it will reset the Colab runtime and you will lose saved variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8hZ3CWhAIpUF",
    "outputId": "60b4e98b-8d83-4573-cbe2-131df190b223"
   },
   "outputs": [],
   "source": [
    "# Import PyTorch and matplotlib\n",
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT-krbNMIw0d"
   },
   "source": [
    "Now let's start making our code device agnostic by setting `device=\"cuda\"` if it's available, otherwise it'll default to `device=\"cpu\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sx2Zpb5sec06",
    "outputId": "88323445-9070-4b3d-a62a-3d924d8d6898"
   },
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1t0Ek0GJq6T"
   },
   "source": [
    "If you've got access to a GPU, the above should've printed out:\n",
    "\n",
    "```\n",
    "Using device: cuda\n",
    "```\n",
    "Otherwise, you'll be using a CPU for the following computations. This is fine for our small dataset but it will take longer for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmilLp3Vec07"
   },
   "source": [
    "### 6.1 Data\n",
    "\n",
    "Let's create some data just like before.\n",
    "\n",
    "First, we'll hard-code some `weight` and `bias` values.\n",
    "\n",
    "Then we'll make a range of numbers between 0 and 1, these will be our `X` values.\n",
    "\n",
    "Finally, we'll use the `X` values, as well as the `weight` and `bias` values to create `y` using the linear regression formula (`y = weight * X + bias`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJqgDWUfec07",
    "outputId": "62d07f54-bb59-4327-a153-79be9ada83d7"
   },
   "outputs": [],
   "source": [
    "# Create weight and bias\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create range values\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# Create X and y (features and labels)\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\n",
    "y = weight * X + bias \n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oaar6rDGLGaQ"
   },
   "source": [
    "Wonderful!\n",
    "\n",
    "Now we've got some data, let's split it into training and test sets.\n",
    "\n",
    "We'll use an 80/20 split with 80% training data and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQoo65evec07",
    "outputId": "80c3f9b7-4d1d-4aef-fc19-7abceaf93eb2"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INW8-McyLeFE"
   },
   "source": [
    "Excellent, let's visualize them to make sure they look okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "gxhc0zCdec07",
    "outputId": "cc3cb921-0d25-4cec-d681-da102547bdb9"
   },
   "outputs": [],
   "source": [
    "# Note: If you've reset your runtime, this function won't work, \n",
    "# you'll have to rerun the cell above where it's instantiated.\n",
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0ycBrxIec07"
   },
   "source": [
    "### 6.2 Building a PyTorch linear model\n",
    "\n",
    "We've got some data, now it's time to make a model.\n",
    "\n",
    "We'll create the same style of model as before except this time, instead of defining the weight and bias parameters of our model manually using `nn.Parameter()`, we'll use [`nn.Linear(in_features, out_features)`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to do it for us.\n",
    "\n",
    "Where `in_features` is the number of dimensions your input data has and `out_features` is the number of dimensions you'd like it to be output to.\n",
    "\n",
    "In our case, both of these are `1` since our data has `1` input feature (`X`) per label (`y`).\n",
    "\n",
    "![comparison of nn.Parameter Linear Regression model and nn.Linear Linear Regression model](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-regression-model-with-nn-Parameter-and-nn-Linear-compared.png)\n",
    "*Creating a linear regression model using `nn.Parameter` versus using `nn.Linear`. There are plenty more examples of where the `torch.nn` module has pre-built computations, including many popular and useful neural network layers.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iOwqtFqec08",
    "outputId": "f7aabd1d-55a7-4f1e-c9b9-9db73d178aef"
   },
   "outputs": [],
   "source": [
    "# Subclass nn.Module to make our model\n",
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear() for creating the model parameters\n",
    "        self.linear_layer = nn.Linear(in_features=1, \n",
    "                                      out_features=1)\n",
    "    \n",
    "    # Define the forward computation (input data x flows through nn.Linear())\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Set the manual seed when creating the model (this isn't always needed but is used for demonstrative purposes, try commenting it out and seeing what happens)\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vLN2pPXNXUs"
   },
   "source": [
    "Notice the outputs of `model_1.state_dict()`, the `nn.Linear()` layer created a random `weight` and `bias` parameter for us.\n",
    "\n",
    "Now let's put our model on the GPU (if it's available).\n",
    "\n",
    "We can change the device our PyTorch objects are on using `.to(device)`.\n",
    "\n",
    "First let's check the model's current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhCvYNpAec08",
    "outputId": "4d0d2c5f-4a9c-44a0-bda5-fd54d16cfa51"
   },
   "outputs": [],
   "source": [
    "# Check model device\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqalUGW5N93K"
   },
   "source": [
    "Wonderful, looks like the model's on the CPU by default.\n",
    "\n",
    "Let's change it to be on the GPU (if it's available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfTYec5Rec08",
    "outputId": "b0d331ba-56b9-4f18-f93d-de7965de41dd"
   },
   "outputs": [],
   "source": [
    "# Set model to GPU if it's available, otherwise it'll default to CPU\n",
    "model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHs0bL5_Oc1k"
   },
   "source": [
    "Nice! Because of our device agnostic code, the above cell will work regardless of whether a GPU is available or not.\n",
    "\n",
    "If you do have access to a CUDA-enabled GPU, you should see an output of something like:\n",
    "\n",
    "```\n",
    "device(type='cuda', index=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwTeP_vkec08"
   },
   "source": [
    "### 6.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPFOV3wUec09"
   },
   "source": [
    "Time to build a training and testing loop.\n",
    "\n",
    "First we'll need a loss function and an optimizer.\n",
    "\n",
    "Let's use the same functions we used earlier, `nn.L1Loss()` and `torch.optim.SGD()`.\n",
    "\n",
    "We'll have to pass the new model's parameters (`model.parameters()`) to the optimizer for it to adjust them during training. \n",
    "\n",
    "The learning rate of `0.01` worked well before too so let's use that again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRgqFKrNec09"
   },
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n",
    "                            lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxuBdoWRP2nU"
   },
   "source": [
    "Beautiful, loss function and optimizer ready, now let's train and evaluate our model using a training and testing loop.\n",
    "\n",
    "The only different thing we'll be doing in this step compared to the previous training loop is putting the data on the target `device`.\n",
    "\n",
    "We've already put our model on the target `device` using `model_1.to(device)`.\n",
    "\n",
    "And we can do the same with the data.\n",
    "\n",
    "That way if the model is on the GPU, the data is on the GPU (and vice versa).\n",
    "\n",
    "Let's step things up a notch this time and set `epochs=1000`.\n",
    "\n",
    "If you need a reminder of the PyTorch training loop steps, see below.\n",
    "\n",
    "<details>\n",
    "    <summary>PyTorch training loop steps</summary>\n",
    "    <ol>\n",
    "        <li><b>Forward pass</b> - The model goes through all of the training data once, performing its\n",
    "            <code>forward()</code> function\n",
    "            calculations (<code>model(x_train)</code>).\n",
    "        </li>\n",
    "        <li><b>Calculate the loss</b> - The model's outputs (predictions) are compared to the ground truth and evaluated\n",
    "            to see how\n",
    "            wrong they are (<code>loss = loss_fn(y_pred, y_train</code>).</li>\n",
    "        <li><b>Zero gradients</b> - The optimizers gradients are set to zero (they are accumulated by default) so they\n",
    "            can be\n",
    "            recalculated for the specific training step (<code>optimizer.zero_grad()</code>).</li>\n",
    "        <li><b>Perform backpropagation on the loss</b> - Computes the gradient of the loss with respect for every model\n",
    "            parameter to\n",
    "            be updated (each parameter\n",
    "            with <code>requires_grad=True</code>). This is known as <b>backpropagation</b>, hence \"backwards\"\n",
    "            (<code>loss.backward()</code>).</li>\n",
    "        <li><b>Step the optimizer (gradient descent)</b> - Update the parameters with <code>requires_grad=True</code>\n",
    "            with respect to the loss\n",
    "            gradients in order to improve them (<code>optimizer.step()</code>).</li>\n",
    "    </ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDOHzX8lec09",
    "outputId": "23ee6dda-7145-463c-e684-d65ba6874757"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs \n",
    "epochs = 1000 \n",
    "\n",
    "# Put data on the available device\n",
    "# Without this, error will happen (not all model/data on device)\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train() # train mode is on by default after construction\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model_1(X_train)\n",
    "\n",
    "    # 2. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_1.eval() # put the model in evaluation mode for testing (inference)\n",
    "    # 1. Forward pass\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test)\n",
    "    \n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt-b2Y131flk"
   },
   "source": [
    "> **Note:** Due to the random nature of machine learning, you will likely get slightly different results (different loss and prediction values) depending on whether your model was trained on CPU or GPU. This is true even if you use the same random seed on either device. If the difference is large, you may want to look for errors, however, if it is small (ideally it is), you can ignore it.\n",
    "\n",
    "Nice! That loss looks pretty low.\n",
    "\n",
    "Let's check the parameters our model has learned and compare them to the original parameters we hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TP_tFn5rec09",
    "outputId": "53b6c53a-1bab-4f13-e09a-c9473200af39"
   },
   "outputs": [],
   "source": [
    "# Find our model's learned parameters\n",
    "from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "pprint(model_1.state_dict())\n",
    "print(\"\\nAnd the original values for weights and bias are:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDZo0vEU1_-1"
   },
   "source": [
    "Ho ho! Now that's pretty darn close to a perfect model.\n",
    "\n",
    "Remember though, in practice, it's rare that you'll know the perfect parameters ahead of time.\n",
    "\n",
    "And if you knew the parameters your model had to learn ahead of time, what would be the fun of machine learning?\n",
    "\n",
    "Plus, in many real-world machine learning problems, the number of parameters can well exceed tens of millions.\n",
    "\n",
    "I don't know about you but I'd rather write code for a computer to figure those out rather than doing it by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBR1qvqhec09"
   },
   "source": [
    "### 6.4 Making predictions\n",
    "\n",
    "Now we've got a trained model, let's turn on it's evaluation mode and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksqG5N5Iec09",
    "outputId": "a0d4a51f-e1d9-4038-fd8a-0bbf4386f36a"
   },
   "outputs": [],
   "source": [
    "# Turn model into evaluation mode\n",
    "model_1.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_1(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtOoVnbi2ysL"
   },
   "source": [
    "If you're making predictions with data on the GPU, you might notice the output of the above has `device='cuda:0'` towards the end. That means the data is on CUDA device 0 (the first GPU your system has access to due to zero-indexing), if you end up using multiple GPUs in the future, this number may be higher. \n",
    "\n",
    "Now let's plot our model's predictions.\n",
    "\n",
    "> **Note:** Many data science libraries such as pandas, matplotlib and NumPy aren't capable of using data that is stored on GPU. So you might run into some issues when trying to use a function from one of these libraries with tensor data not stored on the CPU. To fix this, you can call [`.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html) on your target tensor to return a copy of your target tensor on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "Z4dmfr2bec09",
    "outputId": "dd68d5a7-1733-4385-c1cb-7d7b44085813"
   },
   "outputs": [],
   "source": [
    "# plot_predictions(predictions=y_preds) # -> won't work... data not on CPU\n",
    "\n",
    "# Put data on the CPU and plot it\n",
    "plot_predictions(predictions=y_preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxZa-5-Tec0-"
   },
   "source": [
    "Woah! Look at those red dots, they line up almost perfectly with the green dots. I guess the extra epochs helped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8jCHl1gec0-"
   },
   "source": [
    "### 6.5 Saving and loading a model\n",
    "\n",
    "We're happy with our models predictions, so let's save it to file so it can be used later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcQo4JqL7eSU",
    "outputId": "e43ada0c-c074-4b50-9207-fa01581b1d5f"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk0rvpwV7slc"
   },
   "source": [
    "And just to make sure everything worked well, let's load it back in.\n",
    "\n",
    "We'll:\n",
    "* Create a new instance of the `LinearRegressionModelV2()` class\n",
    "* Load in the model state dict using `torch.nn.Module.load_state_dict()`\n",
    "* Send the new instance of the model to the target device (to ensure our code is device-agnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMnVHzf1ec0-",
    "outputId": "76f10046-cd42-4b39-a372-aa95227828e8"
   },
   "outputs": [],
   "source": [
    "# Instantiate a fresh instance of LinearRegressionModelV2\n",
    "loaded_model_1 = LinearRegressionModelV2()\n",
    "\n",
    "# Load model state dict \n",
    "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\n",
    "loaded_model_1.to(device)\n",
    "\n",
    "print(f\"Loaded model:\\n{loaded_model_1}\")\n",
    "print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv6EMEx99LV2"
   },
   "source": [
    "Now we can evaluate the loaded model to see if its predictions line up with the predictions made prior to saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYODT7ONec0_",
    "outputId": "c8184cd1-595a-43e4-8155-89dcecc4d0b0"
   },
   "outputs": [],
   "source": [
    "# Evaluate loaded model\n",
    "loaded_model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    loaded_model_1_preds = loaded_model_1(X_test)\n",
    "y_preds == loaded_model_1_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M_kcRC89YrZ"
   },
   "source": [
    "Everything adds up! Nice!\n",
    "\n",
    "Well, we've come a long way. You've now built and trained your first two neural network models in PyTorch!\n",
    "\n",
    "Time to practice your skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6rf3hTWec0_"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "All exercises have been inspired from code throughout the notebook.\n",
    "\n",
    "There is one exercise per major section.\n",
    "\n",
    "You should be able to complete them by referencing their specific section.\n",
    "\n",
    "> **Note:** For all exercises, your code should be device agnostic (meaning it could run on CPU or GPU if it's available).\n",
    "\n",
    "1. Create a straight line dataset using the linear regression formula (`weight * X + bias`).\n",
    "  * Set `weight=0.3` and `bias=0.9` there should be at least 100 datapoints total. \n",
    "  * Split the data into 80% training, 20% testing.\n",
    "  * Plot the training and testing data so it becomes visual.\n",
    "2. Build a PyTorch model by subclassing `nn.Module`. \n",
    "  * Inside should be a randomly initialized `nn.Parameter()` with `requires_grad=True`, one for `weights` and one for `bias`. \n",
    "  * Implement the `forward()` method to compute the linear regression function you used to create the dataset in 1. \n",
    "  * Once you've constructed the model, make an instance of it and check its `state_dict()`.\n",
    "  * **Note:** If you'd like to use `nn.Linear()` instead of `nn.Parameter()` you can.\n",
    "3. Create a loss function and optimizer using `nn.L1Loss()` and `torch.optim.SGD(params, lr)` respectively. \n",
    "  * Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\n",
    "  * Write a training loop to perform the appropriate training steps for 300 epochs.\n",
    "  * The training loop should test the model on the test dataset every 20 epochs.\n",
    "4. Make predictions with the trained model on the test data.\n",
    "  * Visualize these predictions against the original training and testing data (**note:** you may need to make sure the predictions are *not* on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).\n",
    "5. Save your trained model's `state_dict()` to file.\n",
    "  * Create a new instance of your model class you made in 2. and load in the `state_dict()` you just saved to it.\n",
    "  * Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.\n",
    "\n",
    "> **Resource:** See the [exercises notebooks templates](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/exercises) and [solutions](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions) on the course GitHub.\n",
    "\n",
    "## Extra-curriculum\n",
    "* Listen to [The Unofficial PyTorch Optimization Loop Song](https://youtu.be/Nutpusq_AFw) (to help remember the steps in a PyTorch training/testing loop).\n",
    "* Read [What is `torch.nn`, really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard for a deeper understanding of how one of the most important modules in PyTorch works. \n",
    "* Spend 10-minutes scrolling through and checking out the [PyTorch documentation cheatsheet](https://pytorch.org/tutorials/beginner/ptcheat.html) for all of the different PyTorch modules you might come across.\n",
    "* Spend 10-minutes reading the [loading and saving documentation on the PyTorch website](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to become more familiar with the different saving and loading options in PyTorch. \n",
    "* Spend 1-2 hours reading/watching the following for an overview of the internals of gradient descent and backpropagation, the two main algorithms that have been working in the background to help our model learn. \n",
    " * [Wikipedia page for gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    " * [Gradient Descent Algorithm — a deep dive](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21) by Robert Kwiatkowski\n",
    " * [Gradient descent, how neural networks learn video](https://youtu.be/IHZwWFHWa-w) by 3Blue1Brown\n",
    " * [What is backpropagation really doing?](https://youtu.be/Ilg3gGewQ5U) video by 3Blue1Brown\n",
    " * [Backpropagation Wikipedia Page](https://en.wikipedia.org/wiki/Backpropagation)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01_pytorch_workflow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
